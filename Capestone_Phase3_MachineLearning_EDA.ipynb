{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE-3\n",
    "\n",
    "#Now make only 1 dataframe of all above 3 csv file using concat/merge /join operation of pandas and start doing EDA .\n",
    "\n",
    "#Do the complete EDA in details to explore the insights of data and write the detailed observations of each analysis . Write code in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "table1_df = pd.read_csv('Table1.csv')\n",
    "table2_df = pd.read_csv('Table2.csv')\n",
    "table3_df = pd.read_csv('Table3.csv')\n",
    "\n",
    "merged_df = pd.merge(table1_df, table2_df, on='Sno', how='inner')\n",
    "merged_df = pd.merge(merged_df, table3_df, on='Sno', how='inner')\n",
    "\n",
    "print(merged_df.head())\n",
    "print(merged_df.info())\n",
    "\n",
    "print(merged_df.describe())\n",
    "\n",
    "print(merged_df.isnull().sum())\n",
    "\n",
    "for column in merged_df.select_dtypes(include='object').columns:\n",
    "    print(column)\n",
    "    print(merged_df[column].value_counts())\n",
    "    print()\n",
    "\n",
    "correlation_matrix = merged_df.corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "merged_df.hist(figsize=(12, 8))\n",
    "plt.suptitle('Histograms of Numerical Features', x=0.5, y=1.02, ha='center', fontsize='large')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for column in merged_df.select_dtypes(include='object').columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(data=merged_df, x=column, order=merged_df[column].value_counts().index)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc3864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PHASE - 4\n",
    "\n",
    "#From the above information . Please do code in python and machine learning. Write the complete Machine learning code to make predictions of price and occasion .Use appropriate models on their label basis. Remember you need to make 2 different predictions: price and occasion   .\n",
    "\n",
    "#Apply all the best techniques of scaling, hyperparameter tuning, avoid underfitting or overfitting (bias/variance)\n",
    "\n",
    "#At the end save the best model and convey on which basis you have chosen that model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e7f1f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Separate features and target variables\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m X_price \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_df\u001b[49m\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOccasion\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m y_price \u001b[38;5;241m=\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m X_occasion \u001b[38;5;241m=\u001b[39m merged_df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOccasion\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import joblib\n",
    "\n",
    "X_price = merged_df.drop(['Price', 'Occasion'], axis=1)\n",
    "y_price = merged_df['Price']\n",
    "X_occasion = merged_df.drop(['Price', 'Occasion'], axis=1)\n",
    "y_occasion = merged_df['Occasion']\n",
    "\n",
    "X_train_price, X_test_price, y_train_price, y_test_price = train_test_split(X_price, y_price, test_size=0.2, random_state=42)\n",
    "X_train_occasion, X_test_occasion, y_train_occasion, y_test_occasion = train_test_split(X_occasion, y_occasion, test_size=0.2, random_state=42)\n",
    "\n",
    "numeric_features = X_price.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "])\n",
    "\n",
    "models = {\n",
    "    'price': RandomForestRegressor(),\n",
    "    'occasion': LogisticRegression()\n",
    "}\n",
    "\n",
    "pipelines = {}\n",
    "for label, model in models.items():\n",
    "    pipelines[label] = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "for label, pipeline in pipelines.items():\n",
    "    if label == 'price':\n",
    "        pipeline.fit(X_train_price, y_train_price)\n",
    "    elif label == 'occasion':\n",
    "        pipeline.fit(X_train_occasion, y_train_occasion)\n",
    "\n",
    "for label, pipeline in pipelines.items():\n",
    "    if label == 'price':\n",
    "        y_pred_price = pipeline.predict(X_test_price)\n",
    "        mse = mean_squared_error(y_test_price, y_pred_price)\n",
    "        print(f'Mean Squared Error for {label}: {mse}')\n",
    "    elif label == 'occasion':\n",
    "        y_pred_occasion = pipeline.predict(X_test_occasion)\n",
    "        accuracy = accuracy_score(y_test_occasion, y_pred_occasion)\n",
    "        print(f'Accuracy Score for {label}: {accuracy}')\n",
    "\n",
    "best_model_label = min(pipelines, key=lambda x: mean_squared_error(y_test_price, pipelines[x].predict(X_test_price)))\n",
    "joblib.dump(pipelines[best_model_label], f'best_model_{best_model_label}.joblib')\n",
    "print(f'Best model ({best_model_label}) saved successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2da67f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
